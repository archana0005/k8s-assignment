Commends Used:
-sudo su -
-kubectl get ns
-cd $home
-yum install git -y
-git clone https://github.com/ashishrpandey/example-voting-app
-ls
-cd example-voting-app/
-ls
-cd k8s-specifications/
-ll
-kubectl apply -f .
-kubectl get pods
-kubectl get all
-kubectl get svc
-kubectl delete pod vote-94849dc97-525jn
-kubectl get rs
-kubectl get deploy
-kubectl scale deploy vote --replicas=3
-kubectl delete pod worker-dd46d7584-lqj97
-kubectl logs worker-dd46d7584-lqj97
-kubectl delete pod db-b54cd94f4
***********************************************************************
Output:
For vote:http://54.169.154.198:31000/
For result:http://54.169.154.198:31001/
***********************************************************************
Observations:
[ec2-user@ip-172-31-24-143 ~]$ sudo su -
Last login: Fri Feb 10 08:30:06 UTC 2023 on pts/0
[root@ip-172-31-24-143 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   4h9m
kube-node-lease   Active   4h9m
kube-public       Active   4h9m
kube-system       Active   4h9m
[root@ip-172-31-24-143 ~]# cd $home
[root@ip-172-31-24-143 ~]# yum install git -y
Loaded plugins: extras_suggestions, langpacks, priorities, update-motd
amzn2-core                                               | 3.7 kB     00:00
11 packages excluded due to repository priority protections
Package git-2.39.1-1.amzn2.0.1.x86_64 already installed and latest version
Nothing to do
[root@ip-172-31-24-143 ~]# git clone https://github.com/ashishrpandey/example-vo                                                                             ting-app
fatal: destination path 'example-voting-app' already exists and is not an empty                                                                              directory.
[root@ip-172-31-24-143 ~]# ls
example-voting-app  kubernetes-training
[root@ip-172-31-24-143 ~]# cd example-voting-app/
[root@ip-172-31-24-143 example-voting-app]# ls
architecture.png               docker-compose.yml  MAINTAINERS  worker
dockercloud.yml                docker-stack.yml    README.md
docker-compose-javaworker.yml  k8s-specifications  result
docker-compose-simple.yml      LICENSE             vote
[root@ip-172-31-24-143 example-voting-app]# cd k8s-specifications/
[root@ip-172-31-24-143 k8s-specifications]# ll
total 36
-rw-r--r-- 1 root root 401 Feb 10 04:33 db-deployment.yaml
-rw-r--r-- 1 root root 146 Feb 10 04:33 db-service.yaml
-rw-r--r-- 1 root root 402 Feb 10 04:33 redis-deployment.yaml
-rw-r--r-- 1 root root 152 Feb 10 04:33 redis-service.yaml
-rw-r--r-- 1 root root 299 Feb 10 04:33 result-deployment.yaml
-rw-r--r-- 1 root root 195 Feb 10 04:33 result-service.yaml
-rw-r--r-- 1 root root 289 Feb 10 04:33 vote-deployment.yaml
-rw-r--r-- 1 root root 192 Feb 10 04:33 vote-service.yaml
-rw-r--r-- 1 root root 292 Feb 10 04:33 worker-deployment.yaml
[root@ip-172-31-24-143 k8s-specifications]# kubectl apply -f .
deployment.apps/db created
service/db created
deployment.apps/redis created
service/redis created
deployment.apps/result created
service/result created
deployment.apps/vote created
service/vote created
deployment.apps/worker created
[root@ip-172-31-24-143 k8s-specifications]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-xcjpk        1/1     Running   0          15s
redis-868d64d78-lm7dl     1/1     Running   0          15s
result-5d57b59f4b-fl4vz   1/1     Running   0          16s
vote-94849dc97-429t9      1/1     Running   0          15s
worker-dd46d7584-628wk    1/1     Running   0          15s
[root@ip-172-31-24-143 k8s-specifications]# kubectl get all
NAME                          READY   STATUS    RESTARTS   AGE
pod/db-b54cd94f4-xcjpk        1/1     Running   0          39s
pod/redis-868d64d78-lm7dl     1/1     Running   0          39s
pod/result-5d57b59f4b-fl4vz   1/1     Running   0          40s
pod/vote-94849dc97-429t9      1/1     Running   0          39s
pod/worker-dd46d7584-628wk    1/1     Running   0          39s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                                       AGE
service/db           ClusterIP   10.100.40.208    <none>        5432/TCP                                                                                      40s
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP                                                                                       3m16s
service/redis        ClusterIP   10.103.155.159   <none>        6379/TCP                                                                                      40s
service/result       NodePort    10.98.212.244    <none>        5001:31001/TCP                                                                                40s
service/vote         NodePort    10.98.240.162    <none>        5000:31000/TCP                                                                                39s

NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/db       1/1     1            1           40s
deployment.apps/redis    1/1     1            1           40s
deployment.apps/result   1/1     1            1           40s
deployment.apps/vote     1/1     1            1           40s
deployment.apps/worker   1/1     1            1           39s

NAME                                DESIRED   CURRENT   READY   AGE
replicaset.apps/db-b54cd94f4        1         1         1       40s
replicaset.apps/redis-868d64d78     1         1         1       40s
replicaset.apps/result-5d57b59f4b   1         1         1       40s
replicaset.apps/vote-94849dc97      1         1         1       40s
replicaset.apps/worker-dd46d7584    1         1         1       39s
[root@ip-172-31-24-143 k8s-specifications]# kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
db           ClusterIP   10.100.40.208    <none>        5432/TCP         72s
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP          3m48s
redis        ClusterIP   10.103.155.159   <none>        6379/TCP         72s
result       NodePort    10.98.212.244    <none>        5001:31001/TCP   72s
vote         NodePort    10.98.240.162    <none>        5000:31000/TCP   71s
[root@ip-172-31-24-143 k8s-specifications]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-xcjpk        1/1     Running   0          90s
redis-868d64d78-lm7dl     1/1     Running   0          90s
result-5d57b59f4b-fl4vz   1/1     Running   0          91s
vote-94849dc97-429t9      1/1     Running   0          90s
worker-dd46d7584-628wk    1/1     Running   0          90s
[root@ip-172-31-24-143 k8s-specifications]# kubectl delete vote-94849dc97-429t9
error: the server doesn't have a resource type "vote-94849dc97-429t9"
[root@ip-172-31-24-143 k8s-specifications]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-xcjpk        1/1     Running   0          4m44s
redis-868d64d78-lm7dl     1/1     Running   0          4m44s
result-5d57b59f4b-fl4vz   1/1     Running   0          4m45s
vote-94849dc97-429t9      1/1     Running   0          4m44s
worker-dd46d7584-628wk    1/1     Running   0          4m44s
[root@ip-172-31-24-143 k8s-specifications]# kubectl delete pod vote-94849dc97-429t9
pod "vote-94849dc97-429t9" deleted

[root@ip-172-31-24-143 k8s-specifications]#
[root@ip-172-31-24-143 k8s-specifications]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-xcjpk        1/1     Running   0          5m45s
redis-868d64d78-lm7dl     1/1     Running   0          5m45s
result-5d57b59f4b-fl4vz   1/1     Running   0          5m46s
vote-94849dc97-8fv78      1/1     Running   0          16s
worker-dd46d7584-628wk    1/1     Running   0          5m45s
***************************************************************************************************************************************************************
Observation 1: Deleting Vote pod
When I delete pod vote-94849dc97-429t9, I observed no change in the front end application.
But the containser ID is changed from 94849dc97-429t9 to vote-94849dc97-8fv78 in the front end app.
By deleting vote pod I observed that the moment I deleted the vote pod , new pod was created and took over the application by not changing any data in the application.
***********************************************************************************************************************************************************************
[root@ip-172-31-24-143 ~]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-7hsj2        1/1     Running   0          35m
redis-868d64d78-lm7dl     1/1     Running   0          46m
result-5d57b59f4b-fl4vz   1/1     Running   2          46m
vote-94849dc97-8fv78      1/1     Running   0          41m
worker-dd46d7584-tkr9z    1/1     Running   2          39m
[root@ip-172-31-24-143 ~]#
[root@ip-172-31-24-143 ~]# kubectl get rs
NAME                DESIRED   CURRENT   READY   AGE
db-b54cd94f4        1         1         1       47m
redis-868d64d78     1         1         1       47m
result-5d57b59f4b   1         1         1       47m
vote-94849dc97      1         1         1       47m
worker-dd46d7584    1         1         1       47m
[root@ip-172-31-24-143 ~]# kubectl get deploy
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
db       1/1     1            1           47m
redis    1/1     1            1           47m
result   1/1     1            1           47m
vote     1/1     1            1           47m
worker   1/1     1            1           47m
[root@ip-172-31-24-143 ~]# kubectl scale deploy vote --replicas=3
deployment.apps/vote scaled
[root@ip-172-31-24-143 ~]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-7hsj2        1/1     Running   0          36m
redis-868d64d78-lm7dl     1/1     Running   0          47m
result-5d57b59f4b-fl4vz   1/1     Running   2          47m
vote-94849dc97-2mxrb      1/1     Running   0          10s
vote-94849dc97-4tzxt      1/1     Running   0          10s
vote-94849dc97-8fv78      1/1     Running   0          42m
worker-dd46d7584-tkr9z    1/1     Running   2          40m
[root@ip-172-31-24-143 ~]# kubectl delete pod vote-94849dc97-2mxrb
pod "vote-94849dc97-2mxrb" deleted

[root@ip-172-31-24-143 ~]#
[root@ip-172-31-24-143 ~]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-7hsj2        1/1     Running   0          37m
redis-868d64d78-lm7dl     1/1     Running   0          48m
result-5d57b59f4b-fl4vz   1/1     Running   2          48m
vote-94849dc97-4tzxt      1/1     Running   0          59s
vote-94849dc97-8fv78      1/1     Running   0          43m
vote-94849dc97-rp85l      1/1     Running   0          21s
worker-dd46d7584-tkr9z    1/1     Running   2          40m
[root@ip-172-31-24-143 ~]#
*******************************************************************************************************************************************************************
Observation 2: Creating replicas and Deleting Vote pod

When I created 3 replicas set for vote pod and tried deleting vote pod,
I observed that the new vote pod is created and no impact in accessing the application and still working the same.
******************************************************************************************************************************************************************
[root@ip-172-31-24-143 k8s-specifications]# ^C
[root@ip-172-31-24-143 k8s-specifications]# kubectl logs worker-dd46d7584-628wk
Waiting for db
Waiting for db
Waiting for db
Connected to db
Found redis at 10.103.155.159
Connecting to redis
Processing vote for 'a' by '9a95ae9c7a36546'
Processing vote for 'a' by '9a95ae9c7a36546'
Processing vote for 'b' by '9a95ae9c7a36546'
Processing vote for 'a' by '9a95ae9c7a36546'
Processing vote for 'b' by '9a95ae9c7a36546'
Processing vote for 'a' by '9a95ae9c7a36546'
Processing vote for 'b' by '9a95ae9c7a36546'
Processing vote for 'a' by '9a95ae9c7a36546'
Processing vote for 'b' by '9a95ae9c7a36546'
Processing vote for 'a' by '9a95ae9c7a36546'
Processing vote for 'b' by '9a95ae9c7a36546'
[root@ip-172-31-24-143 k8s-specifications]# kubectl delete pod worker-dd46d7584-628wk
pod "worker-dd46d7584-628wk" deleted

[root@ip-172-31-24-143 k8s-specifications]#
[root@ip-172-31-24-143 k8s-specifications]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-xcjpk        1/1     Running   0          8m5s
redis-868d64d78-lm7dl     1/1     Running   0          8m5s
result-5d57b59f4b-fl4vz   1/1     Running   0          8m6s
vote-94849dc97-8fv78      1/1     Running   0          2m36s
worker-dd46d7584-tkr9z    1/1     Running   0          16s
[root@ip-172-31-24-143 k8s-specifications]# kubectl logs worker-dd46d7584-tkr9z
Connected to db
Found redis at 10.103.155.159
Connecting to redis
Processing vote for 'a' by '9a95ae9c7a36546'
Processing vote for 'b' by '9a95ae9c7a36546'
[root@ip-172-31-24-143 k8s-specifications]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-xcjpk        1/1     Running   0          8m50s
redis-868d64d78-lm7dl     1/1     Running   0          8m50s
result-5d57b59f4b-fl4vz   1/1     Running   0          8m51s
vote-94849dc97-8fv78      1/1     Running   0          3m21s
worker-dd46d7584-tkr9z    1/1     Running   0          61s
*************************************************************************************************************************
Observation 3: Deleting Worker pod

When I delete the worker pod worker-dd46d7584-628wk, a new worker pod is created worker-dd46d7584-tkr9z.
It did not impact either in the front end application and result.
But when I took the worker logs, I can able to see the changes in logs of before and after deletion of worker pods.
It implies that deleting worker pods doesnt delete the data rather its deleting the logs of the worker application. 
I observed that deleting worker pods results no changes in application and result but the logs of the previous pods are lost.
*************************************************************************************************************************
[root@ip-172-31-24-143 k8s-specifications]# kubectl delete pod db-b54cd94f4-xcjpk
pod "db-b54cd94f4-xcjpk" deleted
[root@ip-172-31-24-143 k8s-specifications]# ^C
[root@ip-172-31-24-143 k8s-specifications]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-xq6z7        1/1     Running   0          47s
redis-868d64d78-lm7dl     1/1     Running   0          10m
result-5d57b59f4b-fl4vz   1/1     Running   1          10m
vote-94849dc97-8fv78      1/1     Running   0          4m36s
worker-dd46d7584-tkr9z    1/1     Running   1          2m16s
[root@ip-172-31-24-143 k8s-specifications]# ^C
[root@ip-172-31-24-143 k8s-specifications]# kubectl logs db-b54cd94f4-xq6z7
The files belonging to this database system will be owned by user "postgres".
This user must also own the server process.

The database cluster will be initialized with locale "en_US.utf8".
The default database encoding has accordingly been set to "UTF8".
The default text search configuration will be set to "english".

Data page checksums are disabled.

fixing permissions on existing directory /var/lib/postgresql/data ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
creating configuration files ... ok
creating template1 database in /var/lib/postgresql/data/base/1 ... ok
initializing pg_authid ... ok
setting password ... ok
initializing dependencies ... ok
creating system views ... ok
loading system objects' descriptions ... ok
creating collations ... ok
creating conversions ... ok
creating dictionaries ... ok
setting privileges on built-in objects ... ok
creating information schema ... ok
loading PL/pgSQL server-side language ... ok
vacuuming database template1 ... ok
copying template1 to template0 ... ok
copying template1 to postgres ... ok
syncing data to disk ... ok

WARNING: enabling "trust" authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.

Success. You can now start the database server using:

    postgres -D /var/lib/postgresql/data
or
    pg_ctl -D /var/lib/postgresql/data -l logfile start

****************************************************
WARNING: No password has been set for the database.
         This will allow anyone with access to the
         Postgres port to access your database. In
         Docker's default configuration, this is
         effectively any other container on the same
         system.

         Use "-e POSTGRES_PASSWORD=password" to set
         it in "docker run".
****************************************************
waiting for server to start....LOG:  database system was shut down at 2023-02-10 08:43:01 UTC
LOG:  MultiXact member wraparound protections are now enabled
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started
 done
server started

/usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/*

LOG:  received fast shutdown request
LOG:  aborting any active transactions
LOG:  autovacuum launcher shutting down
LOG:  shutting down
waiting for server to shut down....LOG:  database system is shut down
 done
server stopped

PostgreSQL init process complete; ready for start up.

LOG:  database system was shut down at 2023-02-10 08:43:02 UTC
LOG:  MultiXact member wraparound protections are now enabled
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started
ERROR:  relation "votes" does not exist at character 38
STATEMENT:  SELECT vote, COUNT(id) AS count FROM votes GROUP BY vote
ERROR:  relation "votes" does not exist at character 38
STATEMENT:  SELECT vote, COUNT(id) AS count FROM votes GROUP BY vote
ERROR:  relation "votes" does not exist at character 38
STATEMENT:  SELECT vote, COUNT(id) AS count FROM votes GROUP BY vote
ERROR:  relation "votes" does not exist at character 38
STATEMENT:  SELECT vote, COUNT(id) AS count FROM votes GROUP BY vote
[root@ip-172-31-24-143 k8s-specifications]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-xq6z7        1/1     Running   0          93s
redis-868d64d78-lm7dl     1/1     Running   0          10m
result-5d57b59f4b-fl4vz   1/1     Running   1          10m
vote-94849dc97-8fv78      1/1     Running   0          5m22s
worker-dd46d7584-tkr9z    1/1     Running   1          3m2s
[root@ip-172-31-24-143 k8s-specifications]# kubectl delete pod db-b54cd94f4-xq6z7
pod "db-b54cd94f4-xq6z7" deleted

^Z
[1]+  Stopped                 kubectl delete pod db-b54cd94f4-xq6z7
[root@ip-172-31-24-143 k8s-specifications]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-7hsj2        1/1     Running   0          36s
redis-868d64d78-lm7dl     1/1     Running   0          12m
result-5d57b59f4b-fl4vz   0/1     Error     1          12m
vote-94849dc97-8fv78      1/1     Running   0          6m37s
worker-dd46d7584-tkr9z    0/1     Error     1          4m17s
[root@ip-172-31-24-143 k8s-specifications]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-7hsj2        1/1     Running   0          83s
redis-868d64d78-lm7dl     1/1     Running   0          12m
result-5d57b59f4b-fl4vz   1/1     Running   2          12m
vote-94849dc97-8fv78      1/1     Running   0          7m24s
worker-dd46d7584-tkr9z    1/1     Running   2          5m4s
[root@ip-172-31-24-143 k8s-specifications]# 
***********************************************************************************************************************************
Observation 4:Deleting db pod
When I delete the db pod, I found no changes in the application and result.
BUt I observed that the worker pod and result pods are restarted. If I delete the db pod again,
I observed that the worker pod and result pod restarts as many times , when I delete dp pod by not affecting the front end application and its result.
***************************************************************************************************************************************
HOW YOU MADE THE RESULT POD work 
- Eventhough I Deleteted db pods working fine for me
- But idealy it shoud not work, If the db pod is deleted then the data will also be deleted.
-In this case make a restart , then the worker pod will search for the new db and it will connect to the new db and new redis pod.
If you still not able to connect to db, try to reconnect to db by socket connection , so that the worker will restart and connect it back.
